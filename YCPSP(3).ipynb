{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13067\\AppData\\Local\\Temp\\ipykernel_22024\\4000706586.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  strength_value = float(row[2])  # 第二列的值\n",
      "C:\\Users\\13067\\AppData\\Local\\Temp\\ipykernel_22024\\4000706586.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sequence = row[1]  # 第三列的序列\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# 读取xlsx文件\n",
    "file_path = \"C:\\\\Users\\\\13067\\\\Desktop\\\\GSM1664928_YeastCorePromoterLibrary_Exp.xlsx\"  # 替换为你的文件路径\n",
    "#yan=\"文件地址\"\n",
    "df = pd.read_excel(file_path, header=0)\n",
    "# 初始化列表来存储序列和强度分类\n",
    "sequences_list = []\n",
    "strengths_list = []\n",
    "# 遍历每一行数据\n",
    "for index, row in df.iterrows():\n",
    "    strength_value = float(row[2])  # 第二列的值\n",
    "    sequence = row[1]  # 第三列的序列\n",
    "    \n",
    "    #处理第二列的数字\n",
    "    if strength_value <= 10:\n",
    "        category = 0  # low\n",
    "    else:\n",
    "        category = 1  # high\n",
    "    # 截取第三列的序列并添加空格\n",
    "    processed_sequence = sequence[0:119]  # 从索引131到220共90bp\n",
    "    processed_sequence_with_spaces = ' '.join(processed_sequence)\n",
    "    \n",
    "    # 将结果添加到列表中\n",
    "    sequences_list.append(processed_sequence_with_spaces.encode('utf-8'))\n",
    "    strengths_list.append(category)\n",
    "# 将列表转换为 NumPy 数组\n",
    "sequences_array = tf.constant(sequences_list)\n",
    "strengths_array = tf.constant(strengths_list)\n",
    "# 创建 tf.data.Dataset 对象\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences_array, strengths_array))\n",
    "# 如果需要预取优化，可以使用 prefetch 方法\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(buffer_size=len(sequences_list))\n",
    "# 如果需要批量优化，可以使用 batch 方法\n",
    "dataset = dataset.batch(batch_size=32)\n",
    "# 计算数据集的总长度\n",
    "dataset_size = sum(1 for _ in dataset)\n",
    "\n",
    "# 计算每个子集的大小\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# 分割数据集\n",
    "data1 = dataset.take(train_size)\n",
    "data2 = dataset.skip(train_size).take(val_size)\n",
    "data3 = dataset.skip(train_size + val_size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 201\n",
    "max_tokens = 6\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "dataset_=[\n",
    "    \"C\",\n",
    "    \"T\",\n",
    "    \"A\",\n",
    "    \"G\"\n",
    "]\n",
    "text_vectorization.adapt(dataset_)\n",
    "int_train_ds = data1.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = data2.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = data3.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "##毕赤酵母序列部分\n",
    "new_data_path = \"C:\\\\Users\\\\13067\\\\Documents\\\\WeChat Files\\\\wxid_uosns6j7qbzm12\\\\FileStorage\\\\File\\\\2025-04\\\\新建 XLSX 工作表.xlsx\"\n",
    "new_df = pd.read_excel(new_data_path,header=0)\n",
    "new_sequences = [\" \".join(seq[:200]) for seq in new_df[\"sequence\"]]\n",
    "new_sequences_vectorized = text_vectorization(new_sequences).numpy()\n",
    "new_ds = tf.data.Dataset.from_tensor_slices(new_sequences_vectorized)\n",
    "new_ds = new_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_2 (Pos  (None, None, 128)        26496     \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_encoder_2 (Tran  (None, None, 128)        297344    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 324,098\n",
      "Trainable params: 324,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "149/149 [==============================] - 3s 14ms/step - loss: 0.6879 - accuracy: 0.6995 - val_loss: 0.5686 - val_accuracy: 0.7656\n",
      "Epoch 2/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.4253 - accuracy: 0.8159 - val_loss: 0.3638 - val_accuracy: 0.8611\n",
      "Epoch 3/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.3957 - accuracy: 0.8352 - val_loss: 0.3699 - val_accuracy: 0.8299\n",
      "Epoch 4/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.3662 - accuracy: 0.8505 - val_loss: 0.3067 - val_accuracy: 0.8681\n",
      "Epoch 5/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.3549 - accuracy: 0.8544 - val_loss: 0.3704 - val_accuracy: 0.8316\n",
      "Epoch 6/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.3430 - accuracy: 0.8580 - val_loss: 0.3432 - val_accuracy: 0.8490\n",
      "Epoch 7/30\n",
      "149/149 [==============================] - 2s 14ms/step - loss: 0.3353 - accuracy: 0.8614 - val_loss: 0.2831 - val_accuracy: 0.8854\n",
      "Epoch 8/30\n",
      "149/149 [==============================] - 2s 14ms/step - loss: 0.3338 - accuracy: 0.8612 - val_loss: 0.2769 - val_accuracy: 0.8872\n",
      "Epoch 9/30\n",
      "149/149 [==============================] - 2s 15ms/step - loss: 0.3209 - accuracy: 0.8639 - val_loss: 0.2700 - val_accuracy: 0.8924\n",
      "Epoch 10/30\n",
      "149/149 [==============================] - 2s 15ms/step - loss: 0.3031 - accuracy: 0.8742 - val_loss: 0.2941 - val_accuracy: 0.8837\n",
      "Epoch 11/30\n",
      "149/149 [==============================] - 2s 15ms/step - loss: 0.3083 - accuracy: 0.8744 - val_loss: 0.3003 - val_accuracy: 0.8628\n",
      "Epoch 12/30\n",
      "149/149 [==============================] - 2s 14ms/step - loss: 0.2939 - accuracy: 0.8723 - val_loss: 0.2772 - val_accuracy: 0.8872\n",
      "Epoch 13/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2954 - accuracy: 0.8756 - val_loss: 0.2836 - val_accuracy: 0.8715\n",
      "Epoch 14/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2942 - accuracy: 0.8744 - val_loss: 0.2425 - val_accuracy: 0.9045\n",
      "Epoch 15/30\n",
      "149/149 [==============================] - 2s 12ms/step - loss: 0.2873 - accuracy: 0.8771 - val_loss: 0.2284 - val_accuracy: 0.9149\n",
      "Epoch 16/30\n",
      "149/149 [==============================] - 2s 12ms/step - loss: 0.2867 - accuracy: 0.8828 - val_loss: 0.2406 - val_accuracy: 0.8837\n",
      "Epoch 17/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2779 - accuracy: 0.8838 - val_loss: 0.2509 - val_accuracy: 0.8906\n",
      "Epoch 18/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2771 - accuracy: 0.8840 - val_loss: 0.2248 - val_accuracy: 0.9028\n",
      "Epoch 19/30\n",
      "149/149 [==============================] - 2s 14ms/step - loss: 0.2824 - accuracy: 0.8796 - val_loss: 0.2479 - val_accuracy: 0.8993\n",
      "Epoch 20/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2694 - accuracy: 0.8867 - val_loss: 0.2255 - val_accuracy: 0.9149\n",
      "Epoch 21/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2743 - accuracy: 0.8815 - val_loss: 0.2112 - val_accuracy: 0.9097\n",
      "Epoch 22/30\n",
      "149/149 [==============================] - 2s 12ms/step - loss: 0.2676 - accuracy: 0.8888 - val_loss: 0.2848 - val_accuracy: 0.8663\n",
      "Epoch 23/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2632 - accuracy: 0.8882 - val_loss: 0.2398 - val_accuracy: 0.8698\n",
      "Epoch 24/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2560 - accuracy: 0.8939 - val_loss: 0.2556 - val_accuracy: 0.8993\n",
      "Epoch 25/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2673 - accuracy: 0.8855 - val_loss: 0.2301 - val_accuracy: 0.8924\n",
      "Epoch 26/30\n",
      "149/149 [==============================] - 2s 12ms/step - loss: 0.2518 - accuracy: 0.8937 - val_loss: 0.2024 - val_accuracy: 0.9219\n",
      "Epoch 27/30\n",
      "149/149 [==============================] - 2s 14ms/step - loss: 0.2517 - accuracy: 0.8930 - val_loss: 0.1841 - val_accuracy: 0.9184\n",
      "Epoch 28/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2513 - accuracy: 0.8893 - val_loss: 0.2442 - val_accuracy: 0.9028\n",
      "Epoch 29/30\n",
      "149/149 [==============================] - 2s 13ms/step - loss: 0.2473 - accuracy: 0.8993 - val_loss: 0.2046 - val_accuracy: 0.9080\n",
      "Epoch 30/30\n",
      "149/149 [==============================] - 2s 15ms/step - loss: 0.2517 - accuracy: 0.8935 - val_loss: 0.1939 - val_accuracy: 0.9167\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.1947 - accuracy: 0.9208\n",
      "Test acc: 0.921\n",
      "49/49 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "sequence_length = 201\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "dense_dim = 128\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=30, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "predictions = model.predict(new_ds)\n",
    "# 输出预测结果（类别概率）\n",
    "predicted_classes = predictions.argmax(axis=-1)  # 获取类别（0或1）\n",
    "predicted_strengths = [\"Low\" if c == 0 else \"High\" for c in predicted_classes]\n",
    "# 将预测结果添加到DataFrame\n",
    "new_df[\"predicted_strength\"] = predicted_strengths\n",
    "new_df[\"probability_high\"] = predictions[:, 1]  # 高表达的概率\n",
    "new_df.to_csv(\"predictions_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
